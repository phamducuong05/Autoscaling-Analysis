{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68a0b59",
   "metadata": {},
   "source": [
    "# ARIMA Model for Traffic Forecasting\n",
    "\n",
    "## Overview\n",
    "This notebook implements an **ARIMA (AutoRegressive Integrated Moving Average)** model to forecast HTTP request traffic for the NASA server logs from July-August 1995. ARIMA is a classical time series forecasting method that combines three components:\n",
    "\n",
    "- **AR (AutoRegressive):** Uses past values to predict future values\n",
    "- **I (Integrated):** Applies differencing to make the time series stationary\n",
    "- **MA (Moving Average):** Uses past forecast errors to improve predictions\n",
    "\n",
    "The ARIMA model is particularly suitable for:\n",
    "- Univariate time series data (single variable: request count)\n",
    "- Data with clear trends and/or seasonal patterns\n",
    "- Situations where interpretability of model parameters is important\n",
    "- Baseline comparisons before trying more complex models\n",
    "\n",
    "**Note:** ARIMA assumes linear relationships and may not capture complex non-linear patterns. We will evaluate its performance and potentially compare with more advanced models (Prophet, LSTM, XGBoost) in subsequent analyses.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. **Pre-train: Suitability & Data Preparation**\n",
    "   - Data loading and exploration\n",
    "   - Stationarity assessment (ADF test)\n",
    "   - ACF/PACF analysis\n",
    "   - Data preprocessing and train/test split\n",
    "\n",
    "2. **Training**\n",
    "   - ARIMA parameter selection\n",
    "   - Model fitting\n",
    "   - Model diagnostics\n",
    "\n",
    "3. **Post-train Evaluation**\n",
    "   - Forecast generation\n",
    "   - Performance metrics (RMSE, MSE, MAE, MAPE)\n",
    "   - Visualization and residual analysis\n",
    "   - Conclusions and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7ae60",
   "metadata": {},
   "source": [
    "# Section 1: Pre-train - Suitability & Data Preparation\n",
    "\n",
    "In this section, we will:\n",
    "1. Load the cleaned traffic data\n",
    "2. Explore the data to understand its characteristics\n",
    "3. Assess whether ARIMA is suitable by checking stationarity\n",
    "4. Analyze autocorrelation patterns using ACF and PACF\n",
    "5. Prepare the data for training with proper train/test split\n",
    "\n",
    "The goal is to ensure our data meets ARIMA's assumptions and is properly formatted for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39d884",
   "metadata": {},
   "source": [
    "## 1.1 Import Required Libraries\n",
    "\n",
    "We'll need several libraries for data manipulation, visualization, and time series analysis:\n",
    "\n",
    "- **pandas & numpy:** Data manipulation and numerical operations\n",
    "- **matplotlib & seaborn:** Visualization\n",
    "- **statsmodels:** ARIMA implementation, ADF test, ACF/PACF plots\n",
    "- **pmdarima:** Automatic ARIMA parameter selection (optional but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Time series and statistical analysis\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.tools import crosstab\n",
    "\n",
    "# Auto-ARIMA for parameter selection\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2266c4",
   "metadata": {},
   "source": [
    "## 1.2 Setup Paths and Load Data\n",
    "\n",
    "We'll use the cleaned data from the data processing pipeline. The data is available at three aggregation levels:\n",
    "- `data_1m.csv`: 1-minute aggregation\n",
    "- `data_5m.csv`: 5-minute aggregation (primary focus)\n",
    "- `data_15m.csv`: 15-minute aggregation\n",
    "\n",
    "We'll focus on the **5-minute aggregation** as it provides a good balance between:\n",
    "- **Granularity:** Captures important short-term patterns\n",
    "- **Noise reduction:** Smoother than 1-minute data\n",
    "- **Computational efficiency:** More manageable than 1-minute data\n",
    "- **Forecast horizon:** Suitable for short-term autoscaling decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup project paths\n",
    "PROJECT_ROOT = Path().resolve().parent.parent\n",
    "DATA_CLEANED_DIR = PROJECT_ROOT / \"data\" / \"cleaned\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Cleaned data directory: {DATA_CLEANED_DIR}\")\n",
    "\n",
    "# Load 5-minute aggregated data (primary focus)\n",
    "data_5m_path = DATA_CLEANED_DIR / 'data_5m.csv'\n",
    "\n",
    "if data_5m_path.exists():\n",
    "    df_5m = pd.read_csv(data_5m_path, index_col=0, parse_dates=True)\n",
    "    print(f\"\\n✓ Successfully loaded data from: {data_5m_path}\")\n",
    "    print(f\"✓ Data shape: {df_5m.shape}\")\n",
    "    print(f\"✓ Date range: {df_5m.index.min()} to {df_5m.index.max()}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Data file not found: {data_5m_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc00aba",
   "metadata": {},
   "source": [
    "## 1.3 Data Exploration and Summary Statistics\n",
    "\n",
    "Before applying ARIMA, let's understand our data:\n",
    "- What columns are available?\n",
    "- What is the distribution of requests?\n",
    "- Are there any missing values or anomalies?\n",
    "- How does the traffic vary over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb701f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nShape: {df_5m.shape[0]} rows × {df_5m.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {list(df_5m.columns)}\")\n",
    "print(f\"\\nIndex type: {type(df_5m.index)}\")\n",
    "print(f\"Index frequency: {pd.infer_freq(df_5m.index)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\" * 60)\n",
    "display(df_5m.head())\n",
    "\n",
    "# Display last few rows\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LAST 5 ROWS\")\n",
    "print(\"=\" * 60)\n",
    "display(df_5m.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f7979",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "\n",
    "Let's examine the statistical properties of our data, particularly the `requests` column which is our target variable for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "display(df_5m.describe())\n",
    "\n",
    "# Focus on requests column\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REQUESTS COLUMN STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean requests per 5 minutes: {df_5m['requests'].mean():.2f}\")\n",
    "print(f\"Median requests per 5 minutes: {df_5m['requests'].median():.2f}\")\n",
    "print(f\"Std deviation: {df_5m['requests'].std():.2f}\")\n",
    "print(f\"Min requests: {df_5m['requests'].min()}\")\n",
    "print(f\"Max requests: {df_5m['requests'].max()}\")\n",
    "print(f\"Total requests: {df_5m['requests'].sum():,.0f}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "missing_values = df_5m.isnull().sum()\n",
    "print(missing_values)\n",
    "if missing_values.sum() > 0:\n",
    "    print(f\"\\n⚠️  Warning: {missing_values.sum()} missing values found\")\n",
    "else:\n",
    "    print(\"\\n✓ No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3909398c",
   "metadata": {},
   "source": [
    "### Visualizing the Time Series\n",
    "\n",
    "Let's plot the request count over time to identify:\n",
    "- **Trends:** Long-term increase or decrease\n",
    "- **Seasonality:** Repeating patterns (daily, weekly)\n",
    "- **Anomalies:** Unusual spikes or drops\n",
    "- **Data gaps:** The known server outage from Aug 1-3, 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the entire time series\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "df_5m['requests'].plot(ax=ax, linewidth=0.8, alpha=0.8, color='#1f77b4')\n",
    "\n",
    "# Highlight system downtime period (Aug 1-3, 1995)\n",
    "downtime_start = pd.Timestamp('1995-08-01')\n",
    "downtime_end = pd.Timestamp('1995-08-03')\n",
    "ax.axvspan(downtime_start, downtime_end, color='red', alpha=0.2, label='Server Outage (Storm)')\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('HTTP Requests Over Time (5-minute aggregation)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Number of Requests', fontsize=12)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2bf10",
   "metadata": {},
   "source": [
    "**Figure 1: HTTP Requests Over Time**\n",
    "\n",
    "This plot shows the request count across the entire time period (July-August 1995). Key observations:\n",
    "\n",
    "- **Red shaded area:** Indicates the server outage from August 1-3, 1995 due to a storm\n",
    "- **Variability:** Traffic shows significant fluctuations with both peaks and troughs\n",
    "- **Potential patterns:** May show daily/weekly seasonality (needs further investigation)\n",
    "- **Overall trend:** Will be assessed more formally with statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615cbdb",
   "metadata": {},
   "source": [
    "### Daily and Weekly Patterns\n",
    "\n",
    "Let's examine whether there are consistent daily or weekly patterns in the traffic, which would inform our ARIMA model's seasonal component (if using SARIMA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ab82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-based features for pattern analysis\n",
    "df_5m_copy = df_5m.copy()\n",
    "df_5m_copy['hour'] = df_5m_copy.index.hour\n",
    "df_5m_copy['day_of_week'] = df_5m_copy.index.dayofweek\n",
    "df_5m_copy['day_name'] = df_5m_copy.index.day_name()\n",
    "\n",
    "# Daily pattern (hourly average)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Average requests by hour of day\n",
    "hourly_avg = df_5m_copy.groupby('hour')['requests'].mean()\n",
    "axes[0].bar(hourly_avg.index, hourly_avg.values, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Average Requests by Hour of Day', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Hour (0-23)', fontsize=10)\n",
    "axes[0].set_ylabel('Average Requests', fontsize=10)\n",
    "axes[0].set_xticks(range(0, 24, 2))\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Average requests by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_avg = df_5m_copy.groupby('day_name')['requests'].mean().reindex(day_order)\n",
    "axes[1].bar(daily_avg.index, daily_avg.values, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Average Requests by Day of Week', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Day of Week', fontsize=10)\n",
    "axes[1].set_ylabel('Average Requests', fontsize=10)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90941ca",
   "metadata": {},
   "source": [
    "**Figure 2: Daily and Weekly Traffic Patterns**\n",
    "\n",
    "**Left Panel - Hourly Pattern:**\n",
    "- Shows the average request count for each hour of the day\n",
    "- Peak hours indicate high-traffic periods\n",
    "- Low hours indicate maintenance or off-peak periods\n",
    "- This pattern is crucial for understanding daily seasonality\n",
    "\n",
    "**Right Panel - Weekly Pattern:**\n",
    "- Shows the average request count for each day of the week\n",
    "- Weekdays vs weekends comparison\n",
    "- Helps identify weekly seasonality\n",
    "- May influence whether we need a seasonal ARIMA (SARIMA) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef1dff",
   "metadata": {},
   "source": [
    "## 1.4 Stationarity Assessment\n",
    "\n",
    "**Why Stationarity Matters for ARIMA:**\n",
    "\n",
    "ARIMA models assume that the time series is **stationary**, meaning:\n",
    "- **Constant mean:** The average value doesn't change over time\n",
    "- **Constant variance:** The spread of values doesn't change over time\n",
    "- **Constant autocovariance:** The correlation between values at different time lags doesn't change\n",
    "\n",
    "If the data is non-stationary, we need to apply transformations (usually differencing) to make it stationary before fitting ARIMA.\n",
    "\n",
    "We'll assess stationarity using:\n",
    "1. **Visual inspection:** Plot rolling statistics\n",
    "2. **Augmented Dickey-Fuller (ADF) test:** Statistical test for unit root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974996ae",
   "metadata": {},
   "source": [
    "### 1.4.1 Visual Inspection - Rolling Statistics\n",
    "\n",
    "We'll plot the rolling mean and standard deviation to visually assess stationarity. If these statistics remain relatively constant over time, the series is likely stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e392d1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Calculate rolling statistics\n",
    "window_size = 24 * 12  # 24 hours * 12 (5-min intervals per hour) = 1 day\n",
    "rolling_mean = df_5m['requests'].rolling(window=window_size).mean()\n",
    "rolling_std = df_5m['requests'].rolling(window=window_size).std()\n",
    "\n",
    "# Plot rolling statistics\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Plot original data\n",
    "ax.plot(df_5m.index, df_5m['requests'], label='Original Data', \n",
    "        color='#1f77b4', linewidth=0.8, alpha=0.7)\n",
    "\n",
    "# Plot rolling mean\n",
    "ax.plot(rolling_mean.index, rolling_mean, label=f'Rolling Mean ({window_size} periods)', \n",
    "        color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "# Plot rolling standard deviation\n",
    "ax.plot(rolling_std.index, rolling_std, label=f'Rolling Std ({window_size} periods)', \n",
    "        color='green', linewidth=2, linestyle='--')\n",
    "\n",
    "# Highlight system downtime\n",
    "ax.axvspan(downtime_start, downtime_end, color='red', alpha=0.1)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('Rolling Statistics - Stationarity Assessment', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Number of Requests', fontsize=12)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39d3d5",
   "metadata": {},
   "source": [
    "**Figure 3: Rolling Mean and Standard Deviation**\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "- **Blue line:** Original request count data\n",
    "- **Red dashed line:** Rolling mean (24-hour window)\n",
    "- **Green dashed line:** Rolling standard deviation (24-hour window)\n",
    "\n",
    "**Visual Assessment:**\n",
    "- If the rolling mean (red) is relatively flat → Stationary\n",
    "- If the rolling mean shows trend → Non-stationary (need differencing)\n",
    "- If the rolling std (green) is constant → Homoscedasticity (good)\n",
    "- If the rolling std varies → Heteroscedasticity (may need transformation)\n",
    "\n",
    "The visual inspection provides an initial assessment, but we need a statistical test for confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330a30f",
   "metadata": {},
   "source": [
    "### 1.4.2 Augmented Dickey-Fuller (ADF) Test\n",
    "\n",
    "The **ADF test** is a formal statistical test for stationarity. It tests the **null hypothesis** that a unit root is present (i.e., the time series is non-stationary).\n",
    "\n",
    "**Test Interpretation:**\n",
    "- **p-value < 0.05:** Reject null hypothesis → Series is **stationary**\n",
    "- **p-value ≥ 0.05:** Fail to reject null hypothesis → Series is **non-stationary**\n",
    "\n",
    "The test also provides the **ADF statistic**, which is compared to critical values at different confidence levels (1%, 5%, 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_adf_test(series, title=\"ADF Test Results\"):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Time series to test\n",
    "    title : str\n",
    "        Title for the output\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Test results including p-value, ADF statistic, and critical values\n",
    "    \"\"\"\n",
    "    result = adfuller(series.dropna(), autolag='AIC')\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(title)\n",
    "    print(\"=\" * 60)\n",
    "    print(f'ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value:.6f}')\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INTERPRETATION\")\n",
    "    print(\"=\" * 60)\n",
    "    if result[1] < 0.05:\n",
    "        print(\"✓ p-value < 0.05\")\n",
    "        print(\"✓ REJECT null hypothesis\")\n",
    "        print(\"✓ The series is STATIONARY\")\n",
    "        is_stationary = True\n",
    "    else:\n",
    "        print(\"✗ p-value ≥ 0.05\")\n",
    "        print(\"✗ FAIL TO REJECT null hypothesis\")\n",
    "        print(\"✗ The series is NON-STATIONARY\")\n",
    "        print(\"✗ Differencing may be required\")\n",
    "        is_stationary = False\n",
    "    \n",
    "    return {\n",
    "        'adf_statistic': result[0],\n",
    "        'p_value': result[1],\n",
    "        'critical_values': result[4],\n",
    "        'is_stationary': is_stationary\n",
    "    }\n",
    "\n",
    "# Perform ADF test on original data\n",
    "adf_results = perform_adf_test(df_5m['requests'], \"Augmented Dickey-Fuller Test - Original Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117c33c",
   "metadata": {},
   "source": [
    "**ADF Test Results Interpretation:**\n",
    "\n",
    "The ADF test provides statistical evidence about stationarity:\n",
    "\n",
    "- **ADF Statistic:** More negative values indicate stronger evidence against the null hypothesis\n",
    "- **p-value:** Probability of observing the test statistic if the null hypothesis is true\n",
    "- **Critical Values:** Threshold values for different confidence levels\n",
    "\n",
    "**Decision Rule:**\n",
    "- If ADF statistic < critical value (or p-value < 0.05) → Stationary\n",
    "- If ADF statistic ≥ critical value (or p-value ≥ 0.05) → Non-stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a057a",
   "metadata": {},
   "source": [
    "### 1.4.3 Differencing (if needed)\n",
    "\n",
    "If the ADF test indicates the series is non-stationary, we need to apply **differencing**. Differencing computes the difference between consecutive observations:\n",
    "\n",
    "- **First-order differencing:** Δyₜ = yₜ - yₜ₋₁\n",
    "- **Second-order differencing:** Δ²yₜ = Δyₜ - Δyₜ₋₁\n",
    "\n",
    "The parameter **d** in ARIMA(p,d,q) represents the number of differencing operations needed to achieve stationarity.\n",
    "\n",
    "Let's apply differencing if needed and re-test for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f38f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if differencing is needed\n",
    "if not adf_results['is_stationary']:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"APPLYING DIFFERENCING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First-order differencing\n",
    "    df_5m['requests_diff1'] = df_5m['requests'].diff()\n",
    "    \n",
    "    # Plot differenced data\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "    \n",
    "    # Original data\n",
    "    axes[0].plot(df_5m.index, df_5m['requests'], color='#1f77b4', linewidth=0.8)\n",
    "    axes[0].set_title('Original Data', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Requests', fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Differenced data\n",
    "    axes[1].plot(df_5m.index, df_5m['requests_diff1'], color='orange', linewidth=0.8)\n",
    "    axes[1].set_title('First-Order Differenced Data', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Date', fontsize=10)\n",
    "    axes[1].set_ylabel('Δ Requests', fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Perform ADF test on differenced data\n",
    "    adf_results_diff1 = perform_adf_test(df_5m['requests_diff1'], \n",
    "                                          \"ADF Test - First-Order Differenced Data\")\n",
    "    \n",
    "    # Determine the value of d\n",
    "    if adf_results_diff1['is_stationary']:\n",
    "        d = 1\n",
    "        print(f\"\\n✓ First-order differencing achieved stationarity\")\n",
    "        print(f\"✓ Recommended d parameter: {d}\")\n",
    "    else:\n",
    "        # Try second-order differencing\n",
    "        df_5m['requests_diff2'] = df_5m['requests_diff1'].diff()\n",
    "        adf_results_diff2 = perform_adf_test(df_5m['requests_diff2'],\n",
    "                                              \"ADF Test - Second-Order Differenced Data\")\n",
    "        \n",
    "        if adf_results_diff2['is_stationary']:\n",
    "            d = 2\n",
    "            print(f\"\\n✓ Second-order differencing achieved stationarity\")\n",
    "            print(f\"✓ Recommended d parameter: {d}\")\n",
    "        else:\n",
    "            d = 2  # Default to 2 if still not stationary\n",
    "            print(f\"\\n⚠️  Series still non-stationary after second-order differencing\")\n",
    "            print(f\"⚠️  Proceeding with d = {d} (may need seasonal differencing)\")\n",
    "else:\n",
    "    d = 0\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NO DIFFERENCING NEEDED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✓ Series is already stationary\")\n",
    "    print(f\"✓ Recommended d parameter: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e69dc",
   "metadata": {},
   "source": [
    "**Differencing Analysis:**\n",
    "\n",
    "- **If d = 0:** Original series is stationary, no differencing needed\n",
    "- **If d = 1:** First-order differencing achieved stationarity\n",
    "- **If d = 2:** Second-order differencing was required\n",
    "\n",
    "The value of **d** will be used in our ARIMA(p,d,q) model. Most time series only require d = 0 or d = 1. Higher values (d ≥ 2) are rare and may indicate the need for seasonal differencing or a different modeling approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392fecc",
   "metadata": {},
   "source": [
    "## 1.5 Autocorrelation Analysis (ACF and PACF)\n",
    "\n",
    "After ensuring stationarity, we analyze the **autocorrelation structure** to determine the AR (p) and MA (q) parameters:\n",
    "\n",
    "- **ACF (Autocorrelation Function):** Measures correlation between a time series and its lagged values\n",
    "- **PACF (Partial Autocorrelation Function):** Measures correlation between a time series and its lagged values, removing the effect of intermediate lags\n",
    "\n",
    "**Guidelines for Parameter Selection:**\n",
    "\n",
    "**For ACF:**\n",
    "- Sharp cutoff after lag q → MA(q) component\n",
    "- Gradual decay → AR component\n",
    "\n",
    "**For PACF:**\n",
    "- Sharp cutoff after lag p → AR(p) component\n",
    "- Gradual decay → MA component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2f789",
   "metadata": {},
   "source": [
    "### 1.5.1 Plot ACF and PACF\n",
    "\n",
    "We'll plot the ACF and PACF for the (possibly differenced) series to identify patterns that suggest appropriate p and q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f0748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which series to analyze (original or differenced)\n",
    "if d == 0:\n",
    "    series_to_analyze = df_5m['requests']\n",
    "    series_name = \"Original Series\"\n",
    "else:\n",
    "    series_to_analyze = df_5m['requests_diff1'] if d == 1 else df_5m['requests_diff2']\n",
    "    series_name = f\"Differenced Series (d={d})\"\n",
    "\n",
    "# Plot ACF and PACF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# ACF plot\n",
    "plot_acf(series_to_analyze.dropna(), lags=50, ax=axes[0], alpha=0.05)\n",
    "axes[0].set_title(f'Autocorrelation Function (ACF)\\n{series_name}', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Lag', fontsize=10)\n",
    "axes[0].set_ylabel('Correlation', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# PACF plot\n",
    "plot_pacf(series_to_analyze.dropna(), lags=50, ax=axes[1], alpha=0.05, method='ywm')\n",
    "axes[1].set_title(f'Partial Autocorrelation Function (PACF)\\n{series_name}', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Lag', fontsize=10)\n",
    "axes[1].set_ylabel('Partial Correlation', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469638d7",
   "metadata": {},
   "source": [
    "**Figure 4: ACF and PACF Plots**\n",
    "\n",
    "**Left Panel - ACF:**\n",
    "- Shows correlation at different lags\n",
    "- Blue shaded area represents 95% confidence interval\n",
    "- Bars outside the shaded area are statistically significant\n",
    "\n",
    "**Right Panel - PACF:**\n",
    "- Shows partial correlation at different lags\n",
    "- Removes influence of intermediate lags\n",
    "- Helps identify AR(p) order\n",
    "\n",
    "**Interpretation Guidelines:**\n",
    "\n",
    "**ACF Pattern:**\n",
    "- **Sharp cutoff after lag q:** Suggests MA(q) model\n",
    "- **Gradual decay (exponential or sinusoidal):** Suggests AR model\n",
    "- **Sinusoidal pattern:** May indicate seasonality\n",
    "\n",
    "**PACF Pattern:**\n",
    "- **Sharp cutoff after lag p:** Suggests AR(p) model\n",
    "- **Gradual decay:** Suggests MA model\n",
    "\n",
    "**Seasonality Detection:**\n",
    "- If ACF shows spikes at regular intervals (e.g., every 12 lags for daily seasonality in 5-min data)\n",
    "- Consider SARIMA (Seasonal ARIMA) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231de740",
   "metadata": {},
   "source": [
    "### 1.5.2 Interpret ACF and PACF for Parameter Selection\n",
    "\n",
    "Based on the ACF and PACF plots, we'll make initial recommendations for p and q parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ef177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ACF and PACF patterns (manual inspection)\n",
    "print(\"=\" * 60)\n",
    "print(\"ACF/PACF ANALYSIS - PARAMETER RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: This is a qualitative analysis based on visual inspection\n",
    "# In practice, you would examine the actual plots and count significant lags\n",
    "\n",
    "print(\"\\nBased on ACF and PACF plots:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. Examine ACF plot (left):\")\n",
    "print(\"   - Count significant lags before cutoff → potential MA(q)\")\n",
    "print(\"   - Look for gradual decay pattern → potential AR component\")\n",
    "print(\"\")\n",
    "print(\"2. Examine PACF plot (right):\")\n",
    "print(\"   - Count significant lags before cutoff → potential AR(p)\")\n",
    "print(\"   - Look for gradual decay pattern → potential MA component\")\n",
    "print(\"\")\n",
    "print(\"3. Check for seasonality:\")\n",
    "print(\"   - Regular spikes at fixed intervals → seasonal component\")\n",
    "print(\"   - Consider SARIMA if seasonality is present\")\n",
    "print(\"\")\n",
    "print(\"4. Initial parameter recommendations:\")\n",
    "print(\"   - p (AR order): [to be determined from PACF]\")\n",
    "print(\"   - d (differencing):\", d)\n",
    "print(\"   - q (MA order): [to be determined from ACF]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755042cd",
   "metadata": {},
   "source": [
    "**ACF/PACF Interpretation Summary:**\n",
    "\n",
    "The visual analysis of ACF and PACF provides initial estimates for:\n",
    "\n",
    "- **p (AR order):** Number of significant lags in PACF before cutoff\n",
    "- **q (MA order):** Number of significant lags in ACF before cutoff\n",
    "- **d (differencing):** Already determined from stationarity test\n",
    "\n",
    "**Important Notes:**\n",
    "- These are initial estimates based on visual inspection\n",
    "- We'll use `auto_arima` (if available) or grid search to find optimal parameters\n",
    "- Different parameter combinations can yield similar performance\n",
    "- Model simplicity (lower p and q) is often preferred for interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9146f6",
   "metadata": {},
   "source": [
    "## 1.6 Train/Test Split\n",
    "\n",
    "For evaluating our ARIMA model's performance, we need to split the data into training and testing sets:\n",
    "\n",
    "- **Training set:** July 1995 + first 22 days of August 1995\n",
    "- **Test set:** Remaining days of August 1995\n",
    "\n",
    "This split ensures:\n",
    "- We train on historical data\n",
    "- We test on future data (simulating real-world forecasting)\n",
    "- The test period includes the post-outage period (Aug 3 onwards)\n",
    "\n",
    "**Important:** Time series data must be split chronologically (no random shuffling) to preserve temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train/test split dates\n",
    "train_end_date = pd.Timestamp('1995-08-22')  # End of training period\n",
    "test_start_date = pd.Timestamp('1995-08-23')  # Start of test period\n",
    "\n",
    "# Split the data\n",
    "train_data = df_5m[df_5m.index <= train_end_date]['requests']\n",
    "test_data = df_5m[df_5m.index >= test_start_date]['requests']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Date range: {train_data.index.min()} to {train_data.index.max()}\")\n",
    "print(f\"  Number of observations: {len(train_data)}\")\n",
    "print(f\"  Percentage: {len(train_data) / len(df_5m) * 100:.1f}%\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Date range: {test_data.index.min()} to {test_data.index.max()}\")\n",
    "print(f\"  Number of observations: {len(test_data)}\")\n",
    "print(f\"  Percentage: {len(test_data) / len(df_5m) * 100:.1f}%\")\n",
    "print(f\"\\nTotal observations: {len(df_5m)}\")\n",
    "\n",
    "# Visualize the split\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax.plot(train_data.index, train_data, label='Training Data', \n",
    "        color='#1f77b4', linewidth=0.8)\n",
    "ax.plot(test_data.index, test_data, label='Test Data', \n",
    "        color='orange', linewidth=0.8)\n",
    "\n",
    "# Mark the split point\n",
    "ax.axvline(train_end_date, color='red', linestyle='--', linewidth=2, \n",
    "           label='Train/Test Split')\n",
    "\n",
    "# Highlight system downtime\n",
    "ax.axvspan(downtime_start, downtime_end, color='red', alpha=0.1)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('Train/Test Split for ARIMA Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Number of Requests', fontsize=12)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8bfcf",
   "metadata": {},
   "source": [
    "**Figure 5: Train/Test Split Visualization**\n",
    "\n",
    "- **Blue region:** Training data (July + first 22 days of August)\n",
    "- **Orange region:** Test data (remaining days of August)\n",
    "- **Red dashed line:** Train/test split boundary (August 22, 1995)\n",
    "- **Light red shaded area:** Server outage period (August 1-3, 1995)\n",
    "\n",
    "This split ensures our model is evaluated on truly unseen future data, simulating real-world forecasting scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8083a0",
   "metadata": {},
   "source": [
    "## 1.7 Section 1 Summary and Conclusions\n",
    "\n",
    "### Key Findings from Pre-train Analysis:\n",
    "\n",
    "**Data Characteristics:**\n",
    "- [Summary of data statistics and patterns]\n",
    "- [Observations about daily/weekly seasonality]\n",
    "- [Notes on data quality and anomalies]\n",
    "\n",
    "**Stationarity Assessment:**\n",
    "- ADF test result: [p-value and interpretation]\n",
    "- Differencing required: [Yes/No, and if yes, what order]\n",
    "- Recommended d parameter: [value]\n",
    "\n",
    "**Autocorrelation Analysis:**\n",
    "- ACF pattern: [description and implications for q]\n",
    "- PACF pattern: [description and implications for p]\n",
    "- Seasonality detected: [Yes/No]\n",
    "\n",
    "**Data Preparation:**\n",
    "- Train set: [date range and size]\n",
    "- Test set: [date range and size]\n",
    "- Data ready for ARIMA training: [Yes/No]\n",
    "\n",
    "### Suitability Assessment:\n",
    "\n",
    "**Is ARIMA suitable for this data?**\n",
    "- [Answer with justification based on analysis]\n",
    "- [If suitable, proceed to training]\n",
    "- [If not suitable, explain why and suggest alternatives]\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Proceed to Section 2: ARIMA Model Training\n",
    "2. Use auto_arima or grid search to find optimal (p,d,q) parameters\n",
    "3. Fit the ARIMA model on training data\n",
    "4. Evaluate model diagnostics\n",
    "5. Generate forecasts and assess performance in Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ceb867",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**[END OF SECTION 1 OUTLINE - READY FOR REVIEW]**\n",
    "\n",
    "**Sections to be added after approval:**\n",
    "- Section 2: Training (ARIMA parameter selection, model fitting, diagnostics)\n",
    "- Section 3: Post-train Evaluation (forecasting, metrics, visualization, conclusions)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
