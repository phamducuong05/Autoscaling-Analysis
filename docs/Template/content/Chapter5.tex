\chapter{Mô hình XGBoost}

\section{Tổng quan mô hình XGBoost}

Mô hình XGBoost (eXtreme Gradient Boosting) là một thuật toán học máy thuộc nhóm Gradient Boosting, được phát triển bởi Tianqi Chen và Carlos Guestrin năm 2016. XGBoost đã trở thành một trong những thuật toán phổ biến nhất trong các cuộc thi dữ liệu và ứng dụng thực tế nhờ hiệu suất vượt trội và khả năng xử lý dữ liệu lớn. Thuật toán này xây dựng một mô hình dự báo mạnh bằng cách kết hợp nhiều mô hình yếu (weak learners), thường là cây quyết định (decision trees), theo cách tuần tự để cải thiện sai số dự báo của các mô hình trước đó.

\section{Phương pháp huấn luyện}

\subsection{Chuẩn bị dữ liệu}

Quá trình chuẩn bị dữ liệu cho mô hình XGBoost bắt đầu bằng việc lựa chọn đặc trưng dựa trên phân tích EDA và kiến thức miền, tập trung vào các nhóm biến cốt lõi: nhóm độ trễ (\texttt{req\_lag\_1}, \texttt{req\_lag\_12}, \texttt{req\_lag\_288}) nhằm nắm bắt tính chu kỳ từ ngắn hạn đến dài hạn; nhóm thống kê trượt (\texttt{rolling\_mean\_1h}, \texttt{rolling\_std\_1h}) để phản ánh xu hướng và độ biến động; nhóm chỉ số lỗi (\texttt{err\_lag\_1}, \texttt{err\_rolling\_mean\_1h}); cùng các biến thời gian tuần hoàn (\texttt{hour\_sin}, \texttt{hour\_cos}, \texttt{day\_of\_week}). Dữ liệu sau đó được phân chia nghiêm ngặt theo trình tự thời gian thành ba tập: \textbf{Training} (02/07--16/08, $\sim$75\%), \textbf{Validation} (17/08--22/08, $\sim$10\%) và \textbf{Test} (23/08--31/08, $\sim$15\%) để mô phỏng chính xác kịch bản dự báo thực tế. Đáng chú ý, khác với LSTM, XGBoost hoạt động dựa trên thuật toán cây quyết định nên không yêu cầu bước chuẩn hóa dữ liệu (normalization), giúp đơn giản hóa quy trình xử lý mà vẫn đảm bảo hiệu quả tính toán.\subsection{Cấu hình mô hình}

Mô hình XGBoost được cấu hình với các siêu tham số sau: \textbf{n\_estimators = 300} là số lượng cây quyết định trong ensemble; \textbf{max\_depth = 6} là độ sâu tối đa của mỗi cây, giúp cân bằng giữa khả năng nắm bắt mẫu hình và tránh overfitting; \textbf{learning\_rate = 0.1} là tốc độ học, điều chỉnh mức độ đóng góp của mỗi cây mới; \textbf{subsample = 0.8} là tỷ lệ mẫu dữ liệu được sử dụng cho mỗi cây (stochastic gradient boosting), giúp giảm overfitting; \textbf{colsample\_bytree = 0.8} là tỷ lệ đặc trưng được sử dụng cho mỗi cây, giúp tăng tính đa dạng của các cây; \textbf{random\_state = 42} để đảm bảo tính tái tạo của kết quả; và \textbf{early\_stopping\_rounds = 50} để dừng huấn luyện khi validation loss không cải thiện trong 50 vòng lặp liên tiếp.

Bảng \ref{tab:xgboost_params} tóm tắt các tham số tối ưu cho mô hình XGBoost.

\begin{table}[H]
    \centering
    \caption{Tham số tối ưu cho mô hình XGBoost}
    \label{tab:xgboost_params}
    \begin{tabular}{lc}
        \toprule
        Tham số & Giá trị \\
        \midrule
        n\_estimators & 300 \\
        max\_depth & 6 \\
        learning\_rate & 0.1 \\
        subsample & 0.8 \\
        colsample\_bytree & 0.8 \\
        random\_state & 42 \\
        early\_stopping\_rounds & 50 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Quá trình huấn luyện}

Quá trình huấn luyện mô hình XGBoost được thực hiện với cơ chế \textit{Early Stopping} trên validation set để tránh overfitting. Mô hình được huấn luyện tối đa 300 cây, nhưng early stopping đã kích hoạt tại vòng lặp thứ 123 khi validation RMSE đạt giá trị tốt nhất là 40.69. Điều này cho thấy mô hình đã hội tụ nhanh chóng và không cần sử dụng hết 300 cây để đạt hiệu quả tối ưu.

Trong quá trình huấn luyện, validation RMSE giảm nhanh trong 30 vòng lặp đầu tiên từ 176.61 xuống 41.03, sau đó giảm chậm hơn và bắt đầu ổn định quanh mức 40.7. Sau vòng lặp thứ 123, validation RMSE bắt đầu tăng nhẹ, cho thấy mô hình bắt đầu overfitting. Early stopping đã kích hoạt và dừng quá trình huấn luyện, giữ lại mô hình với hiệu quả tốt nhất.

\section{Kết quả cho cửa sổ 5 phút}

Sau khi huấn luyện xong mô hình XGBoost tối ưu, chúng tôi đánh giá hiệu quả dự báo trên tập kiểm tra (test set) từ 23/8/1995 đến 31/8/1995. Cửa sổ 5 phút được chọn để phân tích sâu vì nó đạt được sự cân bằng tốt nhất giữa độ chi tiết và độ ổn định trong ba cửa sổ thời gian được đánh giá.

\subsection{Cấu trúc mô hình}

Mô hình XGBoost tối ưu cho cửa sổ 5 phút có kiến trúc: \textbf{123 trees} với early stopping, \textbf{10 features} (lag, rolling statistics, time-based), và \textbf{max\_depth = 6}. Dữ liệu 5 phút đã được làm mượt mà nhờ aggregation, giúp mô hình không cần quá nhiều cây để nắm bắt các mẫu hình.

\subsection{Hiệu quả dự báo}

Khi dự báo trên tập kiểm tra (2,592 quan sát), mô hình XGBoost đạt được các chỉ số hiệu quả sau: \texttt{RMSE = 41.94}, \texttt{MSE = 1759.04}, \texttt{MAE = 31.92}, và \texttt{MAPE = 26.48\%}. So với mô hình ARIMA (RMSE = 121.86, MAE = 99.20, MAPE = 86.26\%) và LSTM (RMSE = 44.59, MAE = 33.86, MAPE = 26.91\%), XGBoost đạt hiệu quả tương đương với LSTM và vượt trội hơn ARIMA.

Bảng \ref{tab:xgboost_performance_5m} tóm tắt hiệu quả dự báo của mô hình XGBoost cho cửa sổ 5 phút.

\begin{table}[H]
    \centering
    \caption{Hiệu quả dự báo XGBoost cho cửa sổ 5 phút}
    \label{tab:xgboost_performance_5m}
    \begin{tabular}{lcccc}
        \toprule
        Cửa sổ & MSE & RMSE & MAE & MAPE (\%) \\
        \midrule
        5m & 1759.04 & 41.94 & 31.92 & 26.48 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Feature Importance}

Một trong những ưu điểm lớn của XGBoost là khả năng xác định mức độ quan trọng của từng đặc trưng (feature importance), giúp hiểu rõ các yếu tố ảnh hưởng đến lưu lượng truy cập. Kết quả phân tích feature importance cho thấy các đặc trưng quan trọng nhất là: \textbf{req\_lag\_1} là đặc trưng quan trọng nhất, cho thấy giá trị request tại thời điểm trước đó có ảnh hưởng lớn nhất đến dự báo; \textbf{rolling\_mean\_1h} đứng thứ hai, phản ánh xu hướng ngắn hạn của traffic; \textbf{req\_lag\_12} đứng thứ ba, cho thấy chu kỳ hàng ngày của traffic; và \textbf{hour\_cos} và \textbf{hour\_sin} cũng có mức độ quan trọng cao, phản ánh tính chu kỳ của thời gian trong ngày.

Kết quả này cho thấy mô hình XGBoost đã học được các mẫu hình quan trọng trong dữ liệu traffic, đặc biệt là tính autoregressive (phụ thuộc vào quá khứ gần) và chu kỳ hàng ngày.

\section{Phân tích và đánh giá}

\subsection{Phân tích quá trình huấn luyện}

Quá trình huấn luyện mô hình XGBoost diễn ra khá ổn định và hội tụ nhanh. Validation RMSE giảm nhanh trong 30 vòng lặp đầu tiên từ 176.61 xuống 41.03, sau đó giảm chậm hơn và bắt đầu ổn định quanh mức 40.7. Early stopping với patience = 50 đã kích hoạt tại vòng lặp thứ 123, dừng quá trình huấn luyện khi validation RMSE đạt giá trị tốt nhất là 40.69.

Cơ chế Early Stopping giúp tránh overfitting và đảm bảo mô hình có khả năng tổng quát hóa tốt trên dữ liệu mới. Việc early stopping kích hoạt sớm (chỉ sử dụng 123/300 cây) cho thấy mô hình đã học được các mẫu hình quan trọng trong dữ liệu mà không cần quá nhiều cây.

\subsection{Phân tích hiệu quả trên các tập dữ liệu}

Để đánh giá mức độ overfitting của mô hình, chúng tôi so sánh hiệu quả trên tập huấn luyện (in-sample), tập validation, và tập kiểm tra (out-of-sample). Kết quả cho thấy mô hình có khả năng tổng quát hóa tốt: hiệu quả trên tập kiểm tra chỉ kém hơn tập huấn luyện một chút.

\begin{table}[H]
    \centering
    \caption{So sánh hiệu quả trên Train, Validation và Test set cho cửa sổ 5 phút}
    \label{tab:xgboost_overfitting}
    \begin{tabular}{lcccc}
        \toprule
        Tập dữ liệu & RMSE & MAE & MAPE (\%) & MSE \\
        \midrule
        Train & 37.71 & 28.85 & 26.49 & 1422.34 \\
        Validation & 40.69 & 30.04 & 47.70 & 1655.63 \\
        Test & 41.94 & 31.92 & 26.48 & 1759.04 \\
        \bottomrule
    \end{tabular}
\end{table}

Tỷ lệ overfitting được tính bằng tỷ lệ giữa \texttt{RMSE test} và \texttt{RMSE train}. Cửa sổ 5 phút có tỷ lệ 1.11x, rất thấp so với ARIMA (2.48x), cho thấy XGBoost có khả năng tổng quát hóa tốt hơn nhiều so với ARIMA. Điều này nhờ cơ chế regularization mạnh mẽ của XGBoost, giúp tránh overfitting trong quá trình huấn luyện.

\subsection{Phân tích dự báo}

Biểu đồ so sánh giữa dự báo của XGBoost và giá trị thực tế cho thấy mô hình nắm bắt được các mẫu hình chính trong dữ liệu traffic. Mô hình dự báo khá chính xác các xu hướng tăng và giảm của traffic, đặc biệt là các chu kỳ hàng ngày với traffic cao vào giờ làm việc (9h-17h) và thấp vào ban đêm (0h-6h).

Tuy nhiên, mô hình vẫn có một số hạn chế. Đầu tiên, dự báo có độ trễ nhẹ so với giá trị thực tế, đặc biệt tại các điểm thay đổi đột ngột. Thứ hai, mô hình đôi khi dự báo quá thấp tại các điểm cực đại (peak traffic) và quá cao tại các điểm cực tiểu (low traffic). Cuối cùng, độ chính xác dự báo giảm nhẹ vào cuối tuần, có thể do dữ liệu training có ít mẫu hình vào cuối tuần hơn so với ngày trong tuần.

\subsection{So sánh với baseline}

Để đánh giá giá trị thực tế của mô hình XGBoost, chúng tôi so sánh với hai mô hình baseline đơn giản: \textbf{Naive baseline} (dự báo bằng giá trị cuối cùng của tập huấn luyện) và \textbf{Mean baseline} (dự báo bằng giá trị trung bình của tập huấn luyện). Kết quả so sánh cho thấy XGBoost vượt trội hơn đáng kể so với cả hai baseline.

XGBoost cải thiện 65.6\% về RMSE so với ARIMA và cải thiện khoảng 70\% so với các baseline đơn giản. Điều này cho thấy XGBoost có giá trị gia tăng thực sự trong việc dự báo lưu lượng truy cập web, đặc biệt là khả năng nắm bắt các mẫu hình phi tuyến tính và đa đặc trưng mà các mô hình thống kê truyền thống không thể làm được.

\subsection{Kết luận}

Mô hình XGBoost đã chứng minh được hiệu quả vượt trội so với ARIMA và tương đương với LSTM trong bài toán dự báo lưu lượng truy cập web. Với RMSE = 41.94, MAE = 31.92, và MAPE = 26.48\%, XGBoost cải thiện đáng kể về độ chính xác dự báo so với ARIMA (RMSE = 121.86, MAE = 99.20, MAPE = 86.26\%) và đạt hiệu quả tương đương với LSTM (RMSE = 44.59, MAE = 33.86, MAPE = 26.91\%). Điều này cho thấy XGBoost có khả năng nắm bắt các mẫu hình phi tuyến tính và đa đặc trưng trong dữ liệu traffic tốt hơn nhiều so với các mô hình thống kê truyền thống.