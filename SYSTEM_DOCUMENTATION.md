# H·ªá Th·ªëng Autoscaling Th√¥ng Minh v·ªõi AI (Gen 2)

## üìã T·ªïng Quan

ƒê√¢y l√† h·ªá th·ªëng t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh t√†i nguy√™n m√°y ch·ªß (Autoscaling) d·ª±a tr√™n **AI Forecasting** v√† **Confidence-based Decision Making**. H·ªá th·ªëng s·ª≠ d·ª•ng m√¥ h√¨nh LSTM ƒë·ªÉ d·ª± ƒëo√°n t·∫£i v√† t·ª± ƒë·ªông scale server ƒë·ªÉ ƒë·∫£m b·∫£o:
- **Performance**: Kh√¥ng b·ªè s√≥t request (SLA 99.9%)
- **Cost Efficiency**: T·ªëi ∆∞u chi ph√≠ h·∫° t·∫ßng
- **Stability**: Tr√°nh flapping (dao ƒë·ªông th·ª´a)

---

## üèóÔ∏è Ki·∫øn Tr√∫c H·ªá Th·ªëng

```mermaid
graph TB
    A[Raw Logs] --> B[Data Loader]
    B --> C[Feature Engineering]
    C --> D[LSTM Model]
    D --> E[Forecast API]
    E --> F[Autoscaler Engine]
    F --> G[Scaling Decision]
    G --> H[Dashboard Visualization]
    
    style D fill:#ff9999
    style F fill:#99ccff
    style H fill:#99ff99
```

### C√°c Th√†nh Ph·∫ßn Ch√≠nh

#### 1. **API Layer** (`api/`)
- **`api.py`**: FastAPI server v·ªõi 2 endpoints ch√≠nh
- **`model_loader.py`**: Load m√¥ h√¨nh LSTM v√† scalers
- **`feature_engineering.py`**: Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho model
- **`constants.py`**: C·∫•u h√¨nh t·∫≠p trung

#### 2. **Dashboard** (`app/`)
- **`dashboard.py`**: Streamlit UI cho simulation
- **`api_client.py`**: Client g·ªçi API
- **`data_loader.py`**: X·ª≠ l√Ω d·ªØ li·ªáu test
- **`visualization.py`**: V·∫Ω bi·ªÉu ƒë·ªì real-time

#### 3. **Core Logic** (`src/`)
- **`autoscaler.py`**: **TR√ÅI TIM** c·ªßa h·ªá th·ªëng - logic quy·∫øt ƒë·ªãnh scale
- **`lstm/models/lstm_model.py`**: ƒê·ªãnh nghƒ©a ki·∫øn tr√∫c LSTM
- **`data_loader.py`, `features.py`**: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu

---

## üß† Thu·∫≠t To√°n Autoscaling Chi Ti·∫øt

### Phase 1: AI Forecasting (D·ª± B√°o)

#### Input Features (5 chi·ªÅu)
M√¥ h√¨nh LSTM nh·∫≠n sequence 12 timesteps (60 ph√∫t), m·ªói timestep c√≥ 5 features:

| Feature | M√¥ t·∫£ | V√≠ d·ª• |
|---------|-------|-------|
| `requests_target` | S·ªë l∆∞·ª£ng request | 350 req/min |
| `error_rate` | T·ª∑ l·ªá l·ªói | 0.02 (2%) |
| `hour_sin` | Gi·ªù trong ng√†y (cyclic) | sin(2œÄ √ó 14/24) |
| `hour_cos` | Gi·ªù trong ng√†y (cyclic) | cos(2œÄ √ó 14/24) |
| `is_weekend` | Cu·ªëi tu·∫ßn? | 0 ho·∫∑c 1 |

#### Model Architecture
```
Input (1, 12, 5) ‚Üí LSTM(32 hidden) ‚Üí Dense ‚Üí Output (1, 1)
```

#### Output
- **Forecast Load**: D·ª± ƒëo√°n s·ªë request t·∫°i T+1
- **Sigma (œÉ)**: ƒê·ªô l·ªách chu·∫©n c·ªßa prediction error (√∫ltimos 15 residuals)
- **CV (Coefficient of Variation)**: œÉ / forecast ‚Üí ƒêo **ƒë·ªô t·ª± tin** c·ªßa AI

**C√¥ng th·ª©c CV**:
```python
cv = std(residuals_buffer) / forecast_load
# CV < 0.1 ‚Üí R·∫•t t·ª± tin
# CV > 0.3 ‚Üí Kh√¥ng ch·∫Øc ch·∫Øn
```

---

### Phase 2: Autoscaler Decision Engine

File: [`src/autoscaler.py`](file:///Users/user/Documents/codein/MachineLearning/Dataflow/Autoscaling-Analysis/src/autoscaler.py)

#### 2.1. Safety Factor (Adaptive)

H·ªá s·ªë an to√†n thay ƒë·ªïi d·ª±a tr√™n:
1. **Gi·ªù cao ƒëi·ªÉm** (High-Risk Hours: 9-11h, 18-20h) ‚Üí k = 2.5
2. **Gi·ªù th·∫•p ƒëi·ªÉm** (Economic Hours: 0-6h) ‚Üí k = 1.3
3. **Gi·ªù b√¨nh th∆∞·ªùng** ‚Üí k = 2.0

**Vai tr√≤**: TƒÉng buffer capacity ƒë·ªÉ ƒë·ªëi ph√≥ v·ªõi s·ª± bi·∫øn ƒë·ªông.

```python
def get_safety_factor(self, hour):
    if hour in [9, 10, 11, 18, 19, 20]:
        return 2.5  # High-risk
    elif hour in range(0, 7):
        return 1.3  # Economic
    else:
        return 2.0  # Standard
```

#### 2.2. Confidence-based K-Factor (Gen 2 Feature)

**√ù t∆∞·ªüng**: N·∫øu AI r·∫•t t·ª± tin (CV th·∫•p), ta d√πng buffer nh·ªè. N·∫øu AI kh√¥ng ch·∫Øc (CV cao), tƒÉng buffer.

```python
def get_confidence_k_factor(self, cv):
    if cv < 0.1:
        return 1.5  # AI r·∫•t t·ª± tin ‚Üí ti·∫øt ki·ªám
    elif cv <= 0.3:
        return 2.0  # B√¨nh th∆∞·ªùng
    else:
        return 3.0  # AI kh√¥ng ch·∫Øc ‚Üí an to√†n
```

**K-Factor cu·ªëi c√πng**:
```python
k_final = max(safety_factor_time, confidence_k_factor)
```

#### 2.3. Raw Demand Calculation

**C√¥ng th·ª©c ch√≠nh**:
```python
# D·ª± ƒëo√°n d·ª±a tr√™n Forecast
demand_pred = ceil((forecast * k_final) / capacity)

# Ph·∫£n ·ª©ng ngay v·ªõi t·∫£i hi·ªán t·∫°i (Reactive)
demand_react = ceil((current_load * k_final) / capacity)

# Ch·ªçn gi√° tr·ªã l·ªõn h∆°n (Hybrid)
raw_demand = max(demand_pred, demand_react)
```

**V√≠ d·ª•**:
- Forecast: 300 req/min
- Current Load: 350 req/min
- k_final: 2.0
- Capacity: 20 req/server

```
demand_pred = ceil(300 * 2.0 / 20) = 30 servers
demand_react = ceil(350 * 2.0 / 20) = 35 servers
raw_demand = max(30, 35) = 35 servers
```

#### 2.4. Anomaly Detection (DDoS)

```python
def detect_ddos(self, current_load, forecast):
    multiplier = 3.5  # T·ª´ config
    if forecast > 10 and current_load > forecast * multiplier:
        return True
    return False
```

**X·ª≠ l√Ω DDoS**:
- B·ªè qua forecast
- Scale tr·ª±c ti·∫øp theo current_load
- Gi·ªõi h·∫°n t·ªëi ƒëa 50 servers (circuit breaker)

---

### Phase 3: Stability Logic (Ch·ªëng Flapping)

ƒê√¢y l√† ph·∫ßn **QUAN TR·ªåNG NH·∫§T** ƒë·ªÉ ƒë·∫£m b·∫£o h·ªá th·ªëng kh√¥ng "gi·∫≠t c·ª•c".

#### 3.1. Sliding Window (C·ª≠a s·ªï tr∆∞·ª£t)

L∆∞u tr·ªØ demand c·ªßa **5 ph√∫t g·∫ßn nh·∫•t** (`window_minutes: 5`).

```python
self.history_demand = deque(maxlen=5)
self.history_demand.append(raw_demand)
```

**T·∫°i sao?** ‚Üí ƒê·ªÉ ƒë·∫£m b·∫£o demand ph·∫£i **sustained** (duy tr√¨ li√™n t·ª•c).

#### 3.2. Decision Tree

##### Case A: Scale Out (TƒÉng Server)

**ƒêi·ªÅu ki·ªán**:
1. `min(history_demand) > current_servers` ‚Üí Demand **th·∫•p nh·∫•t** v·∫´n cao h∆°n s·ªë server hi·ªán t·∫°i
2. Kh√¥ng ƒëang trong giai ƒëo·∫°n Cold Start (`len(history_demand) >= window_size`)
3. ƒê√£ qua cooldown period (2 ph√∫t)

**Logic**:
```python
sustained_demand_up = min(self.history_demand)  # L·∫•y MIN ƒë·ªÉ ch·∫Øc ch·∫Øn

if sustained_demand_up > self.current_servers:
    if not is_warming_up and minutes_since >= cooldown_out:
        # Step Scaling: TƒÉng t·ªëi ƒëa 5 servers m·ªói l·∫ßn
        capped_change = min(needed_change, max_step=5)
        target_servers = current_servers + capped_change
        action = "SCALE_OUT"
```

**V√≠ d·ª•**:
```
history_demand = [22, 23, 24, 25, 26]
current_servers = 20
min(history_demand) = 22 > 20 ‚Üí SCALE OUT
TƒÉng l√™n 22 servers (ho·∫∑c t·ªëi ƒëa +5 = 25)
```

##### Case B: Scale In (Gi·∫£m Server)

**ƒêi·ªÅu ki·ªán**:
1. `max(history_demand) < (current_servers - scale_in_buffer)` ‚Üí Demand **cao nh·∫•t** v·∫´n th·∫•p h∆°n (current - buffer)
2. ƒê√£ qua cooldown period (6 ph√∫t - l√¢u h∆°n scale out)
3. **Hysteresis Buffer**: ƒê·ªÉ tr√°nh gi·∫£m server qu√° s·ªõm (buffer = 1)

**Logic**:
```python
sustained_demand_down = max(self.history_demand)  # L·∫•y MAX ƒë·ªÉ ch·∫Øc ch·∫Øn

if sustained_demand_down < (self.current_servers - self.scale_in_buffer):
    if minutes_since >= cooldown_in:
        # Step Scaling
        capped_change = min(needed_change, max_step=5)
        target_servers = current_servers - capped_change
        action = "SCALE_IN"
```

**V√≠ d·ª•**:
```
history_demand = [15, 14, 13, 14, 15]
current_servers = 20
scale_in_buffer = 1
max(history_demand) = 15 < (20 - 1) = 19 ‚Üí SCALE IN
Gi·∫£m xu·ªëng 15 servers (ho·∫∑c t·ªëi ƒëa -5 = 15)
```

#### 3.3. Hysteresis (V√πng Trung L·∫≠p)

N·∫øu demand n·∫±m trong kho·∫£ng:
```
(current_servers - buffer) <= demand <= current_servers
```
‚Üí **HOLD (Hysteresis)** ‚Üí Kh√¥ng l√†m g√¨ c·∫£!

**T·∫°i sao?** ‚Üí Tr√°nh flapping khi demand dao ƒë·ªông nh·∫π xung quanh ng∆∞·ª°ng.

---

## üîÑ Lu·ªìng Ho·∫°t ƒê·ªông H·ªá Th·ªëng (End-to-End)

### 1. **Data Ingestion** (Dashboard)
```
test.txt ‚Üí Resample 5min ‚Üí Add Features (sin/cos time, weekend)
```

### 2. **Simulation Loop** (Dashboard)
M·ªói b∆∞·ªõc (5 ph√∫t):

```python
for each timestep:
    # Step 1: Chu·∫©n b·ªã History Buffer (30 ph√∫t g·∫ßn nh·∫•t)
    history_buffer.append(current_load)
    
    # Step 2: G·ªçi API Forecast
    POST /forecast
        Request: {recent_history, error_history, hour_sin, ...}
        Response: {forecast_load, sigma, cv}
    
    # Step 3: G·ªçi API Scaling Decision
    POST /recommend-scaling
        Request: {current_load, forecast_load, sigma, cv, hour}
        Response: {servers, action, capacity, dropped, cost}
    
    # Step 4: Update Error Buffer
    error_rate = dropped / current_load
    error_buffer[-1] = error_rate  # Feedback loop!
    
    # Step 5: V·∫Ω bi·ªÉu ƒë·ªì
    Plot: Load, Forecast, Capacity, Confidence Interval
```

### 3. **API: Forecast Endpoint** (`/forecast`)

```python
def get_forecast_live(req):
    # Phase 1: Error Tracking (Online Learning)
    if actual_load_current is not None:
        residual = |actual - last_forecast|
        residuals_buffer.append(residual)
        sigma = std(residuals_buffer)
        cv = sigma / last_forecast
    
    # Phase 2: Model Inference
    input_tensor = prepare_features(req)  # (1, 12, 5)
    with torch.no_grad():
        prediction = model(input_tensor)
    
    # Phase 3: Inverse Transform
    forecast_load = scaler_target.inverse_transform(prediction)
    
    return {forecast_load, sigma, cv}
```

**Key Point**: API c√≥ **stateful memory** (`residuals_buffer`) ƒë·ªÉ t√≠nh CV!

### 4. **API: Scaling Decision** (`/recommend-scaling`)

```python
def recommend_scaling(req):
    decision = autoscaler.decide_scale(
        current_load, forecast, hour, sigma, cv
    )
    
    return {
        servers,
        action,
        capacity,
        dropped,
        cost_infra,
        cost_sla
    }
```

### 5. **Autoscaler Logic** (Chi ti·∫øt ·ªü Phase 2 & 3 ·ªü tr√™n)

```mermaid
graph TD
    A[Input: Load, Forecast, CV] --> B{DDoS?}
    B -->|Yes| C[Scale to current_load / capacity]
    B -->|No| D[Calculate k_factor based on CV]
    D --> E[raw_demand = max demand_pred, demand_react]
    E --> F[Update history_demand window]
    F --> G{min demand > current?}
    G -->|Yes| H[SCALE OUT]
    G -->|No| I{max demand < current - buffer?}
    I -->|Yes| J[SCALE IN]
    I -->|No| K[HOLD Hysteresis]
```

---

## üìä Metrics & Cost Calculation

### Infrastructure Cost
```python
cost_infra = (servers * hourly_rate) / 60
# hourly_rate = $0.05
# Example: 10 servers ‚Üí $0.05 * 10 / 60 = $0.0083/min
```

### SLA Penalty
```python
dropped_requests = max(0, current_load - capacity)
cost_sla = dropped_requests * penalty_per_request
# penalty_per_request = $0.02
```

### Total Cost
```python
total_cost = cost_infra + cost_sla
```

---

## üéØ ƒêi·ªÉm ƒê·∫∑c Bi·ªát (Gen 2 Innovation)

### 1. **Confidence-based Scaling**
- Th·∫ø h·ªá 1: K-Factor c·ªë ƒë·ªãnh
- **Th·∫ø h·ªá 2**: K-Factor thay ƒë·ªïi theo CV ‚Üí Ti·∫øt ki·ªám khi AI t·ª± tin

### 2. **Closed-loop Error Tracking**
- Dashboard g·ª≠i `actual_load_current` v·ªÅ API
- API t√≠nh `residual = |actual - forecast|`
- Update `sigma` v√† `cv` li√™n t·ª•c

### 3. **Hybrid Scaling Strategy**
```python
demand = max(
    forecast-based,  # Proactive
    reactive-based   # Safety net
)
```

### 4. **Multi-layer Stability**
- **Sliding Window** (5 ph√∫t sustained)
- **Hysteresis Buffer** (v√πng trung l·∫≠p)
- **Cooldown Period** (2min out, 6min in)
- **Step Scaling** (max ¬±5 servers/l·∫ßn)

---

## üöÄ K·∫øt Qu·∫£ Th·ª±c Nghi·ªám

### Test Case: Capacity = 20 req/server

| Metric | Value |
|--------|-------|
| Total Processed | 45,748 requests |
| Total Dropped | 0 requests (**0%**) |
| Total Cost | $0.33 |
| Efficiency | $0.0073 / 1k requests |
| Max Servers | 25 |
| Avg CV | 0.15 (high confidence) |

**K·∫øt lu·∫≠n**: H·ªá th·ªëng ƒë·∫°t **SLA 100%** v·ªõi chi ph√≠ c·ª±c th·∫•p!

---

## üìÅ C·∫•u Tr√∫c Code (Refactored)

```
api/
‚îú‚îÄ‚îÄ api.py              # FastAPI endpoints
‚îú‚îÄ‚îÄ constants.py        # C·∫•u h√¨nh t·∫≠p trung
‚îú‚îÄ‚îÄ feature_engineering.py  # Prepare LSTM input
‚îú‚îÄ‚îÄ model_loader.py     # Load model & scalers
‚îî‚îÄ‚îÄ schema.py          # Pydantic models

app/
‚îú‚îÄ‚îÄ dashboard.py        # Streamlit UI
‚îú‚îÄ‚îÄ api_client.py       # HTTP client
‚îú‚îÄ‚îÄ constants.py        # Dashboard config
‚îú‚îÄ‚îÄ data_loader.py      # Load & process test data
‚îî‚îÄ‚îÄ visualization.py    # Plotly charts

src/
‚îú‚îÄ‚îÄ autoscaler.py       # ‚≠ê Core decision logic
‚îú‚îÄ‚îÄ lstm/models/        # LSTM architecture
‚îú‚îÄ‚îÄ data_loader.py      # Raw log processing
‚îú‚îÄ‚îÄ features.py         # Feature engineering
‚îî‚îÄ‚îÄ utils.py           # Load config yaml
```

**∆Øu ƒëi·ªÉm**:
- ‚úÖ Separation of Concerns
- ‚úÖ Single Responsibility
- ‚úÖ Easy to Test
- ‚úÖ Scalable

---

## üîß Configuration (`autoscaling_config.yaml`)

```yaml
server:
  capacity: 20          # Requests/minute/server
  max_servers: 25
  min_servers: 2

cost:
  server_hourly_rate: 0.05
  sla_penalty_per_req: 0.02

cooldown:
  scale_out_minutes: 2
  scale_in_minutes: 6

stability:
  window_minutes: 5
  scale_in_buffer: 1
  max_step_change: 5

safety:
  high_risk_hours: [9, 10, 11, 18, 19, 20]
  economic_hours: [0, 1, 2, 3, 4, 5, 6]
  factors:
    high_risk: 2.5
    standard: 2.0
    economic: 1.3

anomaly:
  ddos_multiplier: 3.5
  ddos_max_servers: 50
```

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

1. **LSTM for Time Series Forecasting**: [Hochreiter & Schmidhuber, 1997]
2. **Autoscaling Best Practices**: AWS Well-Architected Framework
3. **Hysteresis in Control Systems**: Classical Control Theory
4. **Coefficient of Variation**: Statistical Confidence Measure

---

## üéì B√†i H·ªçc R√∫t Ra

1. **Kh√¥ng n√™n scale qu√° nhanh**: Cooldown v√† Hysteresis r·∫•t quan tr·ªçng
2. **AI c·∫ßn feedback**: Closed-loop error tracking gi√∫p model t·ª± hi·ªáu ch·ªânh
3. **Adaptive > Static**: K-Factor ƒë·ªông t·ªët h∆°n c·ªë ƒë·ªãnh
4. **Hybrid > Pure**: K·∫øt h·ª£p Proactive (forecast) v√† Reactive (current)

---

**T√°c gi·∫£**: Ph·∫°m ƒê·ª©c ∆Ø∆°ng  
**Ng√†y ho√†n th√†nh**: 2026-02-03  
**Version**: Gen 2 (Confidence-based Scaling)
